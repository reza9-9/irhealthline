import requests
import json
from datetime import datetime
import time
import re
import random
import xml.etree.ElementTree as ET

class PubMedBot:
    def __init__(self):
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        self.searches_today = 0
        self.max_searches_per_day = 100  # Ø§ÙØ²Ø§ÛŒØ´ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª
        self.email = "your-email@example.com"  # Ø¶Ø±ÙˆØ±ÛŒ Ø¨Ø±Ø§ÛŒ PubMed
        self.api_key = None  # Ø§Ú¯Ø± Ø¯Ø§Ø±ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
        
    def search_meta_analysis(self, topic):
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…ØªØ§-Ø¢Ù†Ø§Ù„ÛŒØ² Ø§Ø² PubMed - Ù†Ø³Ø®Ù‡ ØªØµØ­ÛŒØ­ Ø´Ø¯Ù‡"""
        try:
            if self.searches_today >= self.max_searches_per_day:
                print("âš ï¸ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø§Ø² PubMed Ø±Ø³ÛŒØ¯Ù‡")
                return None
                
            print(f"ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…ØªØ§-Ø¢Ù†Ø§Ù„ÛŒØ² Ø¨Ø±Ø§ÛŒ: {topic}")
            
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡
            search_url = f"{self.base_url}esearch.fcgi"
            params = {
                'db': 'pubmed',
                'term': f'{topic} AND (meta-analysis[pt] OR systematic review[pt])',
                'retmax': 5,  # Ø§ÙØ²Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬
                'retmode': 'json',
                'sort': 'relevance',
                'field': 'title,abstract',
                'datetype': 'pdat',
                'reldate': 3650,  # Ù…Ù‚Ø§Ù„Ø§Øª Û±Û° Ø³Ø§Ù„ Ø§Ø®ÛŒØ±
                'email': self.email
            }
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† API Key Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
            if self.api_key:
                params['api_key'] = self.api_key
                
            print(f"ğŸ“¡ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ PubMed...")
            response = requests.get(search_url, params=params, timeout=30)
            self.searches_today += 1
            
            print(f"ğŸ“Š ÙˆØ¶Ø¹ÛŒØª Ù¾Ø§Ø³Ø®: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                article_ids = data.get('esearchresult', {}).get('idlist', [])
                
                print(f"ğŸ” ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(article_ids)}")
                
                if article_ids:
                    print(f"âœ… {len(article_ids)} Ù…Ù‚Ø§Ù„Ù‡ Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
                    article_details = self.get_article_details(article_ids)
                    if article_details:
                        return article_details
                    else:
                        print("âŒ Ù…Ø´Ú©Ù„ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ù‚Ø§Ù„Ø§Øª")
                        return None
                else:
                    print("ğŸ“­ Ù‡ÛŒÚ† Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ - Ø´Ø§ÛŒØ¯ Ú©ÙˆØ¦Ø±ÛŒ Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±Ø¯")
                    # Ù†Ù…Ø§ÛŒØ´ Ø®Ø·Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯
                    print(f"ğŸ“‹ Ù¾Ø§Ø³Ø® Ú©Ø§Ù…Ù„: {data}")
                    return None
                    
            else:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ PubMed: {response.status_code}")
                print(f"ğŸ“„ Ù…ØªÙ† Ø®Ø·Ø§: {response.text[:200]}")
                return None
                
        except requests.exceptions.Timeout:
            print("âŒ timeout Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ PubMed")
            return None
        except requests.exceptions.ConnectionError:
            print("âŒ Ø®Ø·Ø§ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª")
            return None
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡: {e}")
            return None
    
    def get_article_details(self, article_ids):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª Ú©Ø§Ù…Ù„ Ù…Ù‚Ø§Ù„Ø§Øª - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        try:
            if not article_ids:
                return None
                
            fetch_url = f"{self.base_url}efetch.fcgi"
            params = {
                'db': 'pubmed',
                'id': ','.join(article_ids),
                'retmode': 'xml',
                'rettype': 'abstract',
                'email': self.email
            }
            
            if self.api_key:
                params['api_key'] = self.api_key
                
            print(f"ğŸ“¥ Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª {len(article_ids)} Ù…Ù‚Ø§Ù„Ù‡...")
            response = requests.get(fetch_url, params=params, timeout=45)
            
            if response.status_code == 200:
                articles = self.parse_complete_articles(response.text)
                if articles:
                    print(f"âœ… Ù…ÙˆÙÙ‚ÛŒØª Ø¢Ù…ÛŒØ²: {len(articles)} Ù…Ù‚Ø§Ù„Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
                    return articles
                else:
                    print("âŒ Ù…Ø´Ú©Ù„ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù‚Ø§Ù„Ø§Øª")
                    return None
            else:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù‚Ø§Ù„Ø§Øª: {e}")
            return None
    
    def parse_complete_articles(self, xml_content):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ Ù…Ù‚Ø§Ù„Ø§Øª - Ù†Ø³Ø®Ù‡ Ù…Ù‚Ø§ÙˆÙ… Ø¨Ù‡ Ø®Ø·Ø§"""
        try:
            articles = []
            
            # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ XML
            clean_xml = re.sub(r'xmlns="[^"]+"', '', xml_content)
            root = ET.fromstring(clean_xml)
            
            for article in root.findall('.//PubmedArticle'):
                try:
                    # Ø¹Ù†ÙˆØ§Ù† Ù…Ù‚Ø§Ù„Ù‡
                    title_elem = article.find('.//ArticleTitle')
                    title = title_elem.text if title_elem is not None else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
                    
                    # Ú†Ú©ÛŒØ¯Ù‡ Ú©Ø§Ù…Ù„
                    abstract_text = ""
                    abstract_elems = article.findall('.//AbstractText')
                    for elem in abstract_elems:
                        if elem is not None and elem.text:
                            label = elem.get('Label', '')
                            if label:
                                abstract_text += f"{label}: {elem.text} "
                            else:
                                abstract_text += elem.text + " "
                    
                    abstract = abstract_text.strip() if abstract_text else "Ú†Ú©ÛŒØ¯Ù‡ Ú©Ø§Ù…Ù„ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª"
                    
                    # ÙÙ‚Ø· Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ø§ Ú†Ú©ÛŒØ¯Ù‡ Ú©Ø§Ù…Ù„
                    if len(abstract) < 100:  # Ú†Ú©ÛŒØ¯Ù‡ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡
                        continue
                    
                    # Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†
                    authors = []
                    for author in article.findall('.//Author'):
                        last_name = author.find('LastName')
                        fore_name = author.find('ForeName')
                        if last_name is not None and last_name.text:
                            full_name = last_name.text
                            if fore_name is not None and fore_name.text:
                                full_name = f"{fore_name.text} {full_name}"
                            authors.append(full_name)
                    
                    # Ø³Ø§Ù„ Ø§Ù†ØªØ´Ø§Ø±
                    pub_year = "Ù†Ø§Ù…Ø´Ø®Øµ"
                    year_elem = article.find('.//PubDate/Year')
                    if year_elem is not None and year_elem.text:
                        pub_year = year_elem.text
                    else:
                        # Ø±ÙˆØ´ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ®
                        medline_date = article.find('.//PubDate/MedlineDate')
                        if medline_date is not None and medline_date.text:
                            pub_year = medline_date.text[:4]
                    
                    # Ù…Ø¬Ù„Ù‡
                    journal_elem = article.find('.//Journal/Title')
                    journal = journal_elem.text if journal_elem is not None else "Ù†Ø§Ù…Ø´Ø®Øµ"
                    
                    # DOI
                    doi = "Ù†Ø§Ù…Ø´Ø®Øµ"
                    doi_elems = article.findall('.//ArticleId')
                    for elem in doi_elems:
                        if elem.get('IdType') == 'doi' and elem.text:
                            doi = elem.text
                            break
                    
                    articles.append({
                        'title': title,
                        'abstract': abstract,
                        'authors': authors[:3],  # Û³ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ø§ÙˆÙ„
                        'year': pub_year,
                        'journal': journal,
                        'doi': doi,
                        'source': 'PubMed',
                        'word_count': len(abstract.split())
                    })
                    
                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ù…Ù‚Ø§Ù„Ù‡: {e}")
                    continue
            
            return articles
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ XML: {e}")
            return None

    # Ø¨Ù‚ÛŒÙ‡ Ù…ØªØ¯Ù‡Ø§ Ù…Ø§Ù†Ù†Ø¯ Ù‚Ø¨Ù„ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ù†Ø¯...
